{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In class exercise...\n",
    "* MI is biased in that small sample sizes lead to inaccurate estimates of PDFs, and that can sometimes lead to negative MI values (which should never happen in theory). \n",
    "* A common, and simple, approach, is to compute MI with shuffled condition labels (like randomization tests that we did many weeks back) and then subtract the shuffled MI from the actual MI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import sys\n",
    "epsilon = sys.float_info.epsilon\n",
    "# also define the default font we'll use for figures. \n",
    "fig_font = {'fontname':'Arial', 'size':'20'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First set up two arrays of data...make them correlated to some degree so that there is a reasonably high MI..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[ 0.6102875   1.4428832   2.56669153  3.87376427  4.13863264  5.17660569\n",
      "  6.88041769  7.5184807   8.56319497  9.88971085 10.32586912 11.7982105\n",
      " 12.43468554 13.69990678 14.85570343 15.2605782  16.72368636 17.15431061\n",
      " 18.18266068 19.69450046 20.66694707 21.73904515 22.69494969 23.3226199\n",
      " 24.69812509 25.99900824 26.37183662 27.55383491 28.49266345 29.10925356\n",
      " 30.46019701 31.29703426 32.37751334 33.20437626 34.49754731 35.21546332\n",
      " 36.30404363 37.86686746 38.09235533 39.75594366 40.06719616 41.40401465\n",
      " 42.00811536 43.15400284 44.35581202 45.55180749 46.70050937 47.53308083\n",
      " 48.81546328 49.97919319 50.76328282 51.83004545 52.12524591 53.21055394\n",
      " 54.51673535 55.17593284 56.42137386 57.84354313 58.31945938 59.99322009\n",
      " 60.98615722 61.77542962 62.59970294 63.9751939  64.44860843 65.63349112\n",
      " 66.19228084 67.20056967 68.4917188  69.84942505 70.76725653 71.60746022\n",
      " 72.51858071 73.36852522 74.42860839 75.72373664 76.98828193 77.09423653\n",
      " 78.66410785 79.03281569 80.67092992 81.9116635  82.95618883 83.69651835\n",
      " 84.09501324 85.56329807 86.27757638 87.39187353 88.27046686 89.06317235\n",
      " 90.73523947 91.1610818  92.96793876 93.58563431 94.2545462  95.53501445\n",
      " 96.52557389 97.15492105 98.28609556 99.7144044 ]\n"
     ]
    }
   ],
   "source": [
    "N=100\n",
    "x = np.arange(N)\n",
    "y = x+np.random.rand(N)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(x):\n",
    "    \n",
    "    # figure out unique values of x - can be more than just 0s, 1s\n",
    "    uniquex = np.unique(x)\n",
    "\n",
    "    Hx = 0\n",
    "    for i in np.arange(len(uniquex)):\n",
    "        # probability that x==uniquex[i]\n",
    "        px = np.sum(x==uniquex[i])/len(x)    \n",
    "\n",
    "        # check for px==0 because log2(0) = -inf\n",
    "        if px!=0:\n",
    "            Hx += (-np.sum( px * np.log2(px+epsilon) ))  \n",
    "        else:\n",
    "            print('px is zero for value ', i)\n",
    "        \n",
    "    return Hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condEntropy(x,y):\n",
    "    \n",
    "    Hxy=0\n",
    "    uniquex = np.unique(x)\n",
    "    uniquey = np.unique(y)\n",
    "\n",
    "    # loop over unique elements of y\n",
    "    for i in np.arange(len(uniquey)): \n",
    "\n",
    "        # probability that y==y(i) (prob of each y)\n",
    "        py = np.sum(y==uniquey[i]) / N\n",
    "\n",
    "        # then loop over all possible x's to compute entropy of x at each y\n",
    "        tmp=0\n",
    "        for j in np.arange(len(uniquex)):\n",
    "            px_y = np.sum((x==uniquex[j]) & (y==uniquey[i])) / np.sum(y==uniquey[i])    # e.g. prob x==1 when y==0\n",
    "            tmp += (-( px_y * np.log2(px_y+epsilon) ))                                     # entropy      \n",
    "\n",
    "        # then tally up entropy of x given each specific y multiplied by the probability of that y (py)\n",
    "        Hxy += py*tmp\n",
    "\n",
    "    return Hxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then compute the MI between the arrays. Can do two discrete arrays for simplicity, and import the entropy and conditional entropy functions from the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MI is:  6.643856189774683\n"
     ]
    }
   ],
   "source": [
    "Hx = entropy(x=x)\n",
    "Hxy = condEntropy(x=x,y=y)\n",
    "print('MI is: ', Hx-Hxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now repeat the above operations, but shuffle the data arrays and repeat the analysis many times (~500-1000 times). Plot the distribution of MI values that you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADsdJREFUeJzt3X+s3Xddx/Hny5Wh/JAOdreMtrMjFIQ/ZMwyaogK1MgKSmfc4pCw2jRpjNNAMMo0McbEP7Z/3FyQkWZDOkXGHOAqTnQZTGLIJncwBqPMlTnWa+da2BjCgmbw9o/zKV66y+637fmxfu7zkZyc7/f9/dxz3p/09HW//ZzzPU1VIUnq14/MugFJ0mQZ9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOrZp1AwCnnnpqrV+/ftZtSNIJ5c477/xaVc0tN+5pEfTr169nfn5+1m1I0gklyVeHjHPpRpI6Z9BLUucMeknqnEEvSZ0z6CWpc4OCPskDSb6Q5K4k8632/CS3JLmv3Z/S6klyVZJ9Se5Ocs4kJyBJempHc0b/uqo6u6o2tv1LgVuragNwa9sH2AJsaLedwNXjalaSdPSOZ+lmK7C7be8Gzl9Uv65GbgdWJznjOJ5HknQchgZ9Af+c5M4kO1vt9Kp6CKDdn9bqa4D9i352odUkSTMw9MrY11TVgSSnAbck+fJTjM0StSf9D+TtF8ZOgDPPPHNgG9J0rb/0H2b23A9c9qaZPbf6MuiMvqoOtPuDwEeBc4GHDy/JtPuDbfgCsG7Rj68FDizxmLuqamNVbZybW/arGiRJx2jZoE/y7CTPPbwN/CLwRWAPsK0N2wbc1Lb3ABe3T99sAh47vMQjSZq+IUs3pwMfTXJ4/N9U1ceTfAa4IckO4EHgwjb+ZuCNwD7gcWD72LuWJA22bNBX1f3AK5aofx3YvES9gEvG0p0k6bh5Zawkdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpc4ODPslJST6X5GNt/6wkdyS5L8mHkpzc6s9s+/va8fWTaV2SNMTRnNG/Hdi7aP9y4Iqq2gA8Cuxo9R3Ao1X1YuCKNk6SNCODgj7JWuBNwDVtP8DrgRvbkN3A+W17a9unHd/cxkuSZmDoGf2VwO8D32v7LwC+UVVPtP0FYE3bXgPsB2jHH2vjJUkzsGzQJ/kl4GBV3bm4vMTQGnBs8ePuTDKfZP7QoUODmpUkHb0hZ/SvAd6c5AHgekZLNlcCq5OsamPWAgfa9gKwDqAdfx7wyJEPWlW7qmpjVW2cm5s7rklIkn64ZYO+qv6gqtZW1XrgIuATVfVW4JPABW3YNuCmtr2n7dOOf6KqnnRGL0majuP5HP27gHcm2cdoDf7aVr8WeEGrvxO49PhalCQdj1XLD/l/VXUbcFvbvh84d4kx3wEuHENvkqQx8MpYSeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknq3LJBn+RHk/xbks8nuSfJn7T6WUnuSHJfkg8lObnVn9n297Xj6yc7BUnSUxlyRv8/wOur6hXA2cB5STYBlwNXVNUG4FFgRxu/A3i0ql4MXNHGSZJmZNmgr5Fvtd1ntFsBrwdubPXdwPlte2vbpx3fnCRj61iSdFQGrdEnOSnJXcBB4BbgK8A3quqJNmQBWNO21wD7Adrxx4AXjLNpSdJwg4K+qr5bVWcDa4FzgZctNazdL3X2XkcWkuxMMp9k/tChQ0P7lSQdpaP61E1VfQO4DdgErE6yqh1aCxxo2wvAOoB2/HnAI0s81q6q2lhVG+fm5o6te0nSsoZ86mYuyeq2/WPALwB7gU8CF7Rh24Cb2vaetk87/omqetIZvSRpOlYtP4QzgN1JTmL0i+GGqvpYki8B1yf5U+BzwLVt/LXAXyXZx+hM/qIJ9C1JGmjZoK+qu4FXLlG/n9F6/ZH17wAXjqU7SdJx88pYSeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUueWDfok65J8MsneJPckeXurPz/JLUnua/entHqSXJVkX5K7k5wz6UlIkn64IWf0TwC/W1UvAzYBlyR5OXApcGtVbQBubfsAW4AN7bYTuHrsXUuSBls26Kvqoar6bNv+b2AvsAbYCuxuw3YD57ftrcB1NXI7sDrJGWPvXJI0yFGt0SdZD7wSuAM4vaoegtEvA+C0NmwNsH/Rjy20miRpBgYHfZLnAB8G3lFV33yqoUvUaonH25lkPsn8oUOHhrYhSTpKg4I+yTMYhfwHquojrfzw4SWZdn+w1ReAdYt+fC1w4MjHrKpdVbWxqjbOzc0da/+SpGUM+dRNgGuBvVX1Z4sO7QG2te1twE2L6he3T99sAh47vMQjSZq+VQPGvAZ4G/CFJHe12h8ClwE3JNkBPAhc2I7dDLwR2Ac8Dmwfa8eSpKOybNBX1b+y9Lo7wOYlxhdwyXH2JUkaE6+MlaTOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzi0b9Enel+Rgki8uqj0/yS1J7mv3p7R6klyVZF+Su5OcM8nmJUnLG3JG/37gvCNqlwK3VtUG4Na2D7AF2NBuO4Grx9OmJOlYLRv0VfUp4JEjyluB3W17N3D+ovp1NXI7sDrJGeNqVpJ09I51jf70qnoIoN2f1uprgP2Lxi202pMk2ZlkPsn8oUOHjrENSdJyxv1mbJao1VIDq2pXVW2sqo1zc3NjbkOSdNixBv3Dh5dk2v3BVl8A1i0atxY4cOztSZKO17EG/R5gW9veBty0qH5x+/TNJuCxw0s8kqTZWLXcgCQfBF4LnJpkAfhj4DLghiQ7gAeBC9vwm4E3AvuAx4HtE+hZknQUlg36qnrLDzm0eYmxBVxyvE1JksbHK2MlqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnZtI0Cc5L8m9SfYluXQSzyFJGmbsQZ/kJOAvgC3Ay4G3JHn5uJ9HkjTMJM7ozwX2VdX9VfW/wPXA1gk8jyRpgEkE/Rpg/6L9hVaTJM3Aqgk8Zpao1ZMGJTuBnW33W0nunUAvQ50KfG2Gzz8LzvlpLpeP5WFOqDmPyUqa808MGTSJoF8A1i3aXwscOHJQVe0Cdk3g+Y9akvmq2jjrPqbJOa8MzlkwmaWbzwAbkpyV5GTgImDPBJ5HkjTA2M/oq+qJJL8N/BNwEvC+qrpn3M8jSRpmEks3VNXNwM2TeOwJeVosIU2Zc14ZnLNI1ZPeJ5UkdcSvQJCkzq2ooE+yOsmNSb6cZG+Snzni+FuT3N1un07yiln1Oi7LzXnRuFcl+W6SC6bd47gNmXOS1ya5K8k9Sf5lFn2O04DX9vOS/H2Sz7c5b59Vr+OQ5KXtz+/w7ZtJ3nHEmCS5qn0Vy91JzplVv7M2kTX6p7E/Bz5eVRe0TwQ964jj/wH8fFU9mmQLo7W+V0+7yTFbbs6Hv7bickZvoPfgKeecZDXwHuC8qnowyWmzaHLMlvtzvgT4UlX9cpI54N4kH2hXr59wqupe4Gz4/uv3P4GPHjFsC7Ch3V4NXM2J//f5mKyYoE/y48DPAb8B0F7gP/Air6pPL9q9ndE1ACesIXNufgf4MPCqqTU3IQPn/OvAR6rqwTbm4DR7HLeBcy7guUkCPAd4BHhiim1O0mbgK1X11SPqW4HravRG5O3tXz1nVNVD029xtlbS0s2LgEPAXyb5XJJrkjz7KcbvAP5xOq1NzLJzTrIG+BXgvbNocAKG/Dm/BDglyW1J7kxy8fTbHKshc3438DJGFy9+AXh7VX1vyn1OykXAB5eo+3UszUoK+lXAOcDVVfVK4NvAkl+hnOR1jIL+XdNrbyKGzPlK4F1V9d1pNzchQ+a8Cvhp4E3AG4A/SvKSqXY5XkPm/AbgLuCFjJY83t3+JXBCa8tUbwb+dqnDS9RW5McMV1LQLwALVXVH27+R0V+OH5Dkp4BrgK1V9fUp9jcJQ+a8Ebg+yQPABcB7kpw/vRbHbsicFxitZ3+7qr4GfAo4kd94HzLn7YyWq6qq9jF6P+onp9jjpGwBPltVDy9xbNDXsawEKyboq+q/gP1JXtpKm4EvLR6T5EzgI8Dbqurfp9zi2A2Zc1WdVVXrq2o9o4D4rar6u+l2Oj5D5gzcBPxsklVJnsXoDbq9U2xzrAbO+cFWJ8npwEuB+6fW5OS8haWXbWD01SsXt0/fbAIeW4nr87DCLphKcjajs/WTGb3ItwO/BlBV701yDfCrwOE3dZ440b8cabk5HzH2/cDHqurGKbc5VkPmnOT3Wv17wDVVdeVsuh2PAa/tFwLvB85gtKRxWVX99Wy6HY/2S3o/8KKqeqzVfhO+P+cwem/iPOBxYHtVzc+q31laUUEvSSvRilm6kaSVyqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalz/weg+OsYQERaiwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_shuffle = 500\n",
    "tmp0 = np.zeros(N)\n",
    "tmp1 = np.zeros(N)\n",
    "Hx = np.zeros(num_shuffle)\n",
    "Hxy = np.zeros(num_shuffle)\n",
    "MI = np.zeros(num_shuffle)\n",
    "\n",
    "for i in np.arange(num_shuffle):\n",
    "    for j in np.arange(N):   \n",
    "        if np.random.rand(1) < .5:\n",
    "            tmp0[j] = x[j]\n",
    "            tmp1[j] = y[j]\n",
    "        else:\n",
    "            tmp0[j] = y[j]\n",
    "            tmp1[j] = x[j]\n",
    "    Hx[i] = entropy(x=tmp0)\n",
    "    Hxy[i] = condEntropy(x=tmp0,y=tmp1)\n",
    "    MI[i] = Hx[i] - Hxy[i]\n",
    "\n",
    "#MI = Hx - Hxy\n",
    "\n",
    "plt.hist(MI)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now subtract the mean of the shuffled MI values from your 'real' MI value...this will help correct for any bias that is introduced by a limited sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIt-np.mean(MI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
